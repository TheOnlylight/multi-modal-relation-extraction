{
 "cells": [
  {
   "attachments": {},
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# original examples"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "/home/dijinli/Disk/Workspace/multi-modal-relation-extraction/env/lib/python3.10/site-packages/tqdm/auto.py:22: TqdmWarning: IProgress not found. Please update jupyter and ipywidgets. See https://ipywidgets.readthedocs.io/en/stable/user_install.html\n",
      "  from .autonotebook import tqdm as notebook_tqdm\n",
      "Found cached dataset funsd-layoutlmv3 (/home/dijinli/Disk/Workspace/multi-modal-relation-extraction/data/hf/nielsr___funsd-layoutlmv3/funsd/1.0.0/0e3f4efdfd59aa1c3b4952c517894f7b1fc4d75c12ef01bcc8626a69e41c1bb9)\n",
      "/home/dijinli/Disk/Workspace/multi-modal-relation-extraction/env/lib/python3.10/site-packages/transformers/modeling_utils.py:768: FutureWarning: The `device` argument is deprecated and will be removed in v5 of Transformers.\n",
      "  warnings.warn(\n"
     ]
    },
    {
     "data": {
      "text/plain": [
       "PIL.PngImagePlugin.PngImageFile"
      ]
     },
     "execution_count": 1,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "from transformers import AutoProcessor, AutoModel\n",
    "from datasets import load_dataset\n",
    "cachedir = '/home/dijinli/Disk/Workspace/multi-modal-relation-extraction/data/hf'\n",
    "processor = AutoProcessor.from_pretrained(\"microsoft/layoutlmv3-base\", apply_ocr=False, )\n",
    "model = AutoModel.from_pretrained(\"microsoft/layoutlmv3-base\", cache_dir = cachedir)\n",
    "\n",
    "dataset = load_dataset(\"nielsr/funsd-layoutlmv3\", split=\"train\",cache_dir = cachedir)\n",
    "example = dataset[0]\n",
    "image = example[\"image\"]\n",
    "words = example[\"tokens\"]\n",
    "boxes = example[\"bboxes\"]\n",
    "\n",
    "encoding = processor(image, words, boxes=boxes, return_tensors=\"pt\")\n",
    "\n",
    "outputs = model(**encoding)\n",
    "last_hidden_states = outputs.last_hidden_state\n",
    "type(image)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "dict_keys(['input_ids', 'attention_mask', 'bbox', 'pixel_values'])"
      ]
     },
     "execution_count": 3,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "encoding.keys()"
   ]
  },
  {
   "attachments": {},
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Data process"
   ]
  },
  {
   "attachments": {},
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## load and show"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "/home/dijinli/Disk/Workspace/multi-modal-relation-extraction/env/lib/python3.10/site-packages/tqdm/auto.py:22: TqdmWarning: IProgress not found. Please update jupyter and ipywidgets. See https://ipywidgets.readthedocs.io/en/stable/user_install.html\n",
      "  from .autonotebook import tqdm as notebook_tqdm\n",
      "/home/dijinli/Disk/Workspace/multi-modal-relation-extraction/env/lib/python3.10/site-packages/datasets/arrow_dataset.py:1533: FutureWarning: 'fs' was is deprecated in favor of 'storage_options' in version 2.8.0 and will be removed in 3.0.0.\n",
      "You can remove this warning by passing 'storage_options=fs.storage_options' instead.\n",
      "  warnings.warn(\n"
     ]
    }
   ],
   "source": [
    "from datasets import load_from_disk\n",
    "data = load_from_disk('/home/dijinli/Disk/Workspace/multi-modal-relation-extraction/data/processed')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [],
   "source": [
    "data.features\n",
    "import pickle\n",
    "with open('data.pkl', 'wb') as f:\n",
    "    pickle.dump(data[0], f)"
   ]
  },
  {
   "attachments": {},
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## process"
   ]
  },
  {
   "attachments": {},
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### feature extract"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "metadata": {},
   "outputs": [],
   "source": [
    "from typing import Dict, Iterable, Optional, Union\n",
    "\n",
    "import numpy as np\n",
    "\n",
    "from transformers.utils import is_vision_available\n",
    "from transformers.utils.generic import TensorType\n",
    "\n",
    "from transformers.image_processing_utils import BaseImageProcessor, BatchFeature, get_size_dict\n",
    "from transformers.image_transforms import normalize, rescale, resize, to_channel_dimension_format, to_pil_image\n",
    "from transformers.image_utils import (\n",
    "    IMAGENET_STANDARD_MEAN,\n",
    "    IMAGENET_STANDARD_STD,\n",
    "    ChannelDimension,\n",
    "    ImageInput,\n",
    "    PILImageResampling,\n",
    "    infer_channel_dimension_format,\n",
    "    is_batched,\n",
    "    to_numpy_array,\n",
    "    valid_images,\n",
    ")\n",
    "from transformers.utils import is_pytesseract_available, logging, requires_backends\n",
    "\n",
    "\n",
    "if is_vision_available():\n",
    "    import PIL\n",
    "\n",
    "# soft dependency\n",
    "if is_pytesseract_available():\n",
    "    import pytesseract\n",
    "\n",
    "logger = logging.get_logger(__name__)\n",
    "\n",
    "\n",
    "def normalize_box(box, width, height):\n",
    "    return [\n",
    "        int(1000 * (box[0] / width)),\n",
    "        int(1000 * (box[1] / height)),\n",
    "        int(1000 * (box[2] / width)),\n",
    "        int(1000 * (box[3] / height)),\n",
    "    ]\n",
    "\n",
    "\n",
    "def apply_tesseract(image: np.ndarray, lang: Optional[str], tesseract_config: Optional[str]):\n",
    "    \"\"\"Applies Tesseract OCR on a document image, and returns recognized words + normalized bounding boxes.\"\"\"\n",
    "\n",
    "    # apply OCR\n",
    "    pil_image = to_pil_image(image)\n",
    "    image_width, image_height = pil_image.size\n",
    "    data = pytesseract.image_to_data(pil_image, lang=lang, output_type=\"dict\", config=tesseract_config)\n",
    "    words, left, top, width, height = data[\"text\"], data[\"left\"], data[\"top\"], data[\"width\"], data[\"height\"]\n",
    "\n",
    "    # filter empty words and corresponding coordinates\n",
    "    irrelevant_indices = [idx for idx, word in enumerate(words) if not word.strip()]\n",
    "    words = [word for idx, word in enumerate(words) if idx not in irrelevant_indices]\n",
    "    left = [coord for idx, coord in enumerate(left) if idx not in irrelevant_indices]\n",
    "    top = [coord for idx, coord in enumerate(top) if idx not in irrelevant_indices]\n",
    "    width = [coord for idx, coord in enumerate(width) if idx not in irrelevant_indices]\n",
    "    height = [coord for idx, coord in enumerate(height) if idx not in irrelevant_indices]\n",
    "\n",
    "    # turn coordinates into (left, top, left+width, top+height) format\n",
    "    actual_boxes = []\n",
    "    for x, y, w, h in zip(left, top, width, height):\n",
    "        actual_box = [x, y, x + w, y + h]\n",
    "        actual_boxes.append(actual_box)\n",
    "\n",
    "    # finally, normalize the bounding boxes\n",
    "    normalized_boxes = []\n",
    "    for box in actual_boxes:\n",
    "        normalized_boxes.append(normalize_box(box, image_width, image_height))\n",
    "\n",
    "    assert len(words) == len(normalized_boxes), \"Not as many words as there are bounding boxes\"\n",
    "\n",
    "    return words, normalized_boxes\n",
    "\n",
    "\n",
    "def flip_channel_order(image: np.ndarray, data_format: Optional[ChannelDimension] = None) -> np.ndarray:\n",
    "    input_data_format = infer_channel_dimension_format(image)\n",
    "    if input_data_format == ChannelDimension.LAST:\n",
    "        image = image[..., ::-1]\n",
    "    elif input_data_format == ChannelDimension.FIRST:\n",
    "        image = image[:, ::-1, ...]\n",
    "    else:\n",
    "        raise ValueError(f\"Unsupported channel dimension: {input_data_format}\")\n",
    "\n",
    "    if data_format is not None:\n",
    "        image = to_channel_dimension_format(image, data_format)\n",
    "    return image\n",
    "\n",
    "\n",
    "class LayoutLMREImageProcessor(BaseImageProcessor):\n",
    "    r\"\"\"\n",
    "    Constructs a LayoutLMRE image processor.\n",
    "    Args:\n",
    "        do_resize (`bool`, *optional*, defaults to `True`):\n",
    "            Whether to resize the image's (height, width) dimensions to `(size[\"height\"], size[\"width\"])`. Can be\n",
    "            overridden by `do_resize` in `preprocess`.\n",
    "        size (`Dict[str, int]` *optional*, defaults to `{\"height\": 224, \"width\": 224}`):\n",
    "            Size of the image after resizing. Can be overridden by `size` in `preprocess`.\n",
    "        resample (`PILImageResampling`, *optional*, defaults to `PILImageResampling.BILINEAR`):\n",
    "            Resampling filter to use if resizing the image. Can be overridden by `resample` in `preprocess`.\n",
    "        do_rescale (`bool`, *optional*, defaults to `True`):\n",
    "            Whether to rescale the image's pixel values by the specified `rescale_value`. Can be overridden by\n",
    "            `do_rescale` in `preprocess`.\n",
    "        rescale_factor (`float`, *optional*, defaults to 1 / 255):\n",
    "            Value by which the image's pixel values are rescaled. Can be overridden by `rescale_factor` in\n",
    "            `preprocess`.\n",
    "        do_normalize (`bool`, *optional*, defaults to `True`):\n",
    "            Whether to normalize the image. Can be overridden by the `do_normalize` parameter in the `preprocess`\n",
    "            method.\n",
    "        image_mean (`Iterable[float]` or `float`, *optional*, defaults to `IMAGENET_STANDARD_MEAN`):\n",
    "            Mean to use if normalizing the image. This is a float or list of floats the length of the number of\n",
    "            channels in the image. Can be overridden by the `image_mean` parameter in the `preprocess` method.\n",
    "        image_std (`Iterable[float]` or `float`, *optional*, defaults to `IMAGENET_STANDARD_STD`):\n",
    "            Standard deviation to use if normalizing the image. This is a float or list of floats the length of the\n",
    "            number of channels in the image. Can be overridden by the `image_std` parameter in the `preprocess` method.\n",
    "        apply_ocr (`bool`, *optional*, defaults to `True`):\n",
    "            Whether to apply the Tesseract OCR engine to get words + normalized bounding boxes. Can be overridden by\n",
    "            the `apply_ocr` parameter in the `preprocess` method.\n",
    "        ocr_lang (`str`, *optional*):\n",
    "            The language, specified by its ISO code, to be used by the Tesseract OCR engine. By default, English is\n",
    "            used. Can be overridden by the `ocr_lang` parameter in the `preprocess` method.\n",
    "        tesseract_config (`str`, *optional*):\n",
    "            Any additional custom configuration flags that are forwarded to the `config` parameter when calling\n",
    "            Tesseract. For example: '--psm 6'. Can be overridden by the `tesseract_config` parameter in the\n",
    "            `preprocess` method.\n",
    "    \"\"\"\n",
    "\n",
    "    model_input_names = [\"pixel_values\"]\n",
    "\n",
    "    def __init__(\n",
    "        self,\n",
    "        do_resize: bool = True,\n",
    "        size: Dict[str, int] = None,\n",
    "        resample: PILImageResampling = PILImageResampling.BILINEAR,\n",
    "        do_rescale: bool = True,\n",
    "        rescale_value: float = 1 / 255,\n",
    "        do_normalize: bool = True,\n",
    "        image_mean: Union[float, Iterable[float]] = None,\n",
    "        image_std: Union[float, Iterable[float]] = None,\n",
    "        apply_ocr: bool = True,\n",
    "        ocr_lang: Optional[str] = None,\n",
    "        tesseract_config: Optional[str] = \"\",\n",
    "        **kwargs\n",
    "    ) -> None:\n",
    "        super().__init__(**kwargs)\n",
    "        size = size if size is not None else {\"height\": 224, \"width\": 224}\n",
    "        size = get_size_dict(size)\n",
    "\n",
    "        self.do_resize = do_resize\n",
    "        self.size = size\n",
    "        self.resample = resample\n",
    "        self.do_rescale = do_rescale\n",
    "        self.rescale_factor = rescale_value\n",
    "        self.do_normalize = do_normalize\n",
    "        self.image_mean = image_mean if image_mean is not None else IMAGENET_STANDARD_MEAN\n",
    "        self.image_std = image_std if image_std is not None else IMAGENET_STANDARD_STD\n",
    "        self.apply_ocr = apply_ocr\n",
    "        self.ocr_lang = ocr_lang\n",
    "        self.tesseract_config = tesseract_config\n",
    "\n",
    "    def resize(\n",
    "        self,\n",
    "        image: np.ndarray,\n",
    "        size: Dict[str, int],\n",
    "        resample: PILImageResampling = PILImageResampling.BILINEAR,\n",
    "        data_format: Optional[Union[str, ChannelDimension]] = None,\n",
    "        **kwargs\n",
    "    ) -> np.ndarray:\n",
    "        \"\"\"\n",
    "        Resize an image to (size[\"height\"], size[\"width\"]) dimensions.\n",
    "        Args:\n",
    "            image (`np.ndarray`):\n",
    "                Image to resize.\n",
    "            size (`Dict[str, int]`):\n",
    "                Size of the output image.\n",
    "            resample (`PILImageResampling`, *optional*, defaults to `PILImageResampling.BILINEAR`):\n",
    "                Resampling filter to use when resiizing the image.\n",
    "            data_format (`str` or `ChannelDimension`, *optional*):\n",
    "                The channel dimension format of the image. If not provided, it will be the same as the input image.\n",
    "        \"\"\"\n",
    "        size = get_size_dict(size)\n",
    "        if \"height\" not in size or \"width\" not in size:\n",
    "            raise ValueError(f\"The size dictionary must contain the keys 'height' and 'width'. Got {size.keys()}\")\n",
    "        output_size = (size[\"height\"], size[\"width\"])\n",
    "        return resize(image, size=output_size, resample=resample, data_format=data_format, **kwargs)\n",
    "\n",
    "    def rescale(\n",
    "        self,\n",
    "        image: np.ndarray,\n",
    "        scale: Union[int, float],\n",
    "        data_format: Optional[Union[str, ChannelDimension]] = None,\n",
    "        **kwargs\n",
    "    ) -> np.ndarray:\n",
    "        \"\"\"\n",
    "        Rescale an image by a scale factor. image = image * scale.\n",
    "        Args:\n",
    "            image (`np.ndarray`):\n",
    "                Image to rescale.\n",
    "            scale (`int` or `float`):\n",
    "                Scale to apply to the image.\n",
    "            data_format (`str` or `ChannelDimension`, *optional*):\n",
    "                The channel dimension format of the image. If not provided, it will be the same as the input image.\n",
    "        \"\"\"\n",
    "        return rescale(image, scale=scale, data_format=data_format, **kwargs)\n",
    "\n",
    "    def normalize(\n",
    "        self,\n",
    "        image: np.ndarray,\n",
    "        mean: Union[float, Iterable[float]],\n",
    "        std: Union[float, Iterable[float]],\n",
    "        data_format: Optional[Union[str, ChannelDimension]] = None,\n",
    "        **kwargs\n",
    "    ) -> np.ndarray:\n",
    "        \"\"\"\n",
    "        Normalize an image.\n",
    "        Args:\n",
    "            image (`np.ndarray`):\n",
    "                Image to normalize.\n",
    "            mean (`float` or `Iterable[float]`):\n",
    "                Mean values to be used for normalization.\n",
    "            std (`float` or `Iterable[float]`):\n",
    "                Standard deviation values to be used for normalization.\n",
    "            data_format (`str` or `ChannelDimension`, *optional*):\n",
    "                The channel dimension format of the image. If not provided, it will be the same as the input image.\n",
    "        \"\"\"\n",
    "        return normalize(image, mean=mean, std=std, data_format=data_format, **kwargs)\n",
    "\n",
    "    def preprocess(\n",
    "        self,\n",
    "        images: ImageInput,\n",
    "        do_resize: bool = None,\n",
    "        size: Dict[str, int] = None,\n",
    "        resample=None,\n",
    "        do_rescale: bool = None,\n",
    "        rescale_factor: float = None,\n",
    "        do_normalize: bool = None,\n",
    "        image_mean: Union[float, Iterable[float]] = None,\n",
    "        image_std: Union[float, Iterable[float]] = None,\n",
    "        apply_ocr: bool = None,\n",
    "        ocr_lang: Optional[str] = None,\n",
    "        tesseract_config: Optional[str] = None,\n",
    "        return_tensors: Optional[Union[str, TensorType]] = None,\n",
    "        data_format: ChannelDimension = ChannelDimension.FIRST,\n",
    "        **kwargs,\n",
    "    ) -> PIL.Image.Image:\n",
    "        \"\"\"\n",
    "        Preprocess an image or batch of images.\n",
    "        Args:\n",
    "            images (`ImageInput`):\n",
    "                Image to preprocess.\n",
    "            do_resize (`bool`, *optional*, defaults to `self.do_resize`):\n",
    "                Whether to resize the image.\n",
    "            size (`Dict[str, int]`, *optional*, defaults to `self.size`):\n",
    "                Desired size of the output image after applying `resize`.\n",
    "            resample (`int`, *optional*, defaults to `self.resample`):\n",
    "                Resampling filter to use if resizing the image. This can be one of the `PILImageResampling` filters.\n",
    "                Only has an effect if `do_resize` is set to `True`.\n",
    "            do_rescale (`bool`, *optional*, defaults to `self.do_rescale`):\n",
    "                Whether to rescale the image pixel values between [0, 1].\n",
    "            rescale_factor (`float`, *optional*, defaults to `self.rescale_factor`):\n",
    "                Rescale factor to apply to the image pixel values. Only has an effect if `do_rescale` is set to `True`.\n",
    "            do_normalize (`bool`, *optional*, defaults to `self.do_normalize`):\n",
    "                Whether to normalize the image.\n",
    "            image_mean (`float` or `Iterable[float]`, *optional*, defaults to `self.image_mean`):\n",
    "                Mean values to be used for normalization. Only has an effect if `do_normalize` is set to `True`.\n",
    "            image_std (`float` or `Iterable[float]`, *optional*, defaults to `self.image_std`):\n",
    "                Standard deviation values to be used for normalization. Only has an effect if `do_normalize` is set to\n",
    "                `True`.\n",
    "            apply_ocr (`bool`, *optional*, defaults to `self.apply_ocr`):\n",
    "                Whether to apply the Tesseract OCR engine to get words + normalized bounding boxes.\n",
    "            ocr_lang (`str`, *optional*, defaults to `self.ocr_lang`):\n",
    "                The language, specified by its ISO code, to be used by the Tesseract OCR engine. By default, English is\n",
    "                used.\n",
    "            tesseract_config (`str`, *optional*, defaults to `self.tesseract_config`):\n",
    "                Any additional custom configuration flags that are forwarded to the `config` parameter when calling\n",
    "                Tesseract.\n",
    "            return_tensors (`str` or `TensorType`, *optional*):\n",
    "                The type of tensors to return. Can be one of:\n",
    "                    - Unset: Return a list of `np.ndarray`.\n",
    "                    - `TensorType.TENSORFLOW` or `'tf'`: Return a batch of type `tf.Tensor`.\n",
    "                    - `TensorType.PYTORCH` or `'pt'`: Return a batch of type `torch.Tensor`.\n",
    "                    - `TensorType.NUMPY` or `'np'`: Return a batch of type `np.ndarray`.\n",
    "                    - `TensorType.JAX` or `'jax'`: Return a batch of type `jax.numpy.ndarray`.\n",
    "            data_format (`ChannelDimension` or `str`, *optional*, defaults to `ChannelDimension.FIRST`):\n",
    "                The channel dimension format for the output image. Can be one of:\n",
    "                    - `ChannelDimension.FIRST`: image in (num_channels, height, width) format.\n",
    "                    - `ChannelDimension.LAST`: image in (height, width, num_channels) format.\n",
    "        \"\"\"\n",
    "        do_resize = do_resize if do_resize is not None else self.do_resize\n",
    "        size = size if size is not None else self.size\n",
    "        size = get_size_dict(size)\n",
    "        resample = resample if resample is not None else self.resample\n",
    "        do_rescale = do_rescale if do_rescale is not None else self.do_rescale\n",
    "        rescale_factor = rescale_factor if rescale_factor is not None else self.rescale_factor\n",
    "        do_normalize = do_normalize if do_normalize is not None else self.do_normalize\n",
    "        image_mean = image_mean if image_mean is not None else self.image_mean\n",
    "        image_std = image_std if image_std is not None else self.image_std\n",
    "        apply_ocr = apply_ocr if apply_ocr is not None else self.apply_ocr\n",
    "        ocr_lang = ocr_lang if ocr_lang is not None else self.ocr_lang\n",
    "        tesseract_config = tesseract_config if tesseract_config is not None else self.tesseract_config\n",
    "\n",
    "        if not is_batched(images):\n",
    "            images = [images]\n",
    "\n",
    "        if not valid_images(images):\n",
    "            raise ValueError(\n",
    "                \"Invalid image type. Must be of type PIL.Image.Image, numpy.ndarray, \"\n",
    "                \"torch.Tensor, tf.Tensor or jax.ndarray.\"\n",
    "            )\n",
    "\n",
    "        if do_resize and size is None:\n",
    "            raise ValueError(\"Size must be specified if do_resize is True.\")\n",
    "\n",
    "        if do_rescale and rescale_factor is None:\n",
    "            raise ValueError(\"Rescale factor must be specified if do_rescale is True.\")\n",
    "\n",
    "        if do_normalize and (image_mean is None or image_std is None):\n",
    "            raise ValueError(\"If do_normalize is True, image_mean and image_std must be specified.\")\n",
    "\n",
    "        # All transformations expect numpy arrays.\n",
    "        images = [to_numpy_array(image) for image in images]\n",
    "\n",
    "        # Tesseract OCR to get words + normalized bounding boxes\n",
    "        if apply_ocr:\n",
    "            requires_backends(self, \"pytesseract\")\n",
    "            words_batch = []\n",
    "            boxes_batch = []\n",
    "            for image in images:\n",
    "                words, boxes = apply_tesseract(image, ocr_lang, tesseract_config)\n",
    "                words_batch.append(words)\n",
    "                boxes_batch.append(boxes)\n",
    "\n",
    "        if do_resize:\n",
    "            images = [self.resize(image=image, size=size, resample=resample) for image in images]\n",
    "\n",
    "        if do_rescale:\n",
    "            images = [self.rescale(image=image, scale=rescale_factor) for image in images]\n",
    "\n",
    "        if do_normalize:\n",
    "            images = [self.normalize(image=image, mean=image_mean, std=image_std) for image in images]\n",
    "\n",
    "        # flip color channels from RGB to BGR (as Detectron2 requires this)\n",
    "        images = [to_channel_dimension_format(image, data_format) for image in images]\n",
    "\n",
    "        data = BatchFeature(data={\"pixel_values\": images}, tensor_type=return_tensors)\n",
    "\n",
    "        if apply_ocr:\n",
    "            data[\"words\"] = words_batch\n",
    "            data[\"boxes\"] = boxes_batch\n",
    "        return data"
   ]
  },
  {
   "attachments": {},
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### total process\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 45,
   "metadata": {},
   "outputs": [],
   "source": [
    "from typing import List, Optional, Union\n",
    "\n",
    "from transformers.processing_utils import ProcessorMixin\n",
    "from transformers.tokenization_utils_base import BatchEncoding, PaddingStrategy, PreTokenizedInput, TextInput, TruncationStrategy\n",
    "from transformers.utils import TensorType\n",
    "\n",
    "class LayoutLMREProcessor():\n",
    "    r\"\"\"\n",
    "    Constructs a LayoutLMRE processor which combines a LayoutLMRE feature extractor and a LayoutLMRE tokenizer into a\n",
    "    single processor.\n",
    "    [`LayoutLMREProcessor`] offers all the functionalities you need to prepare data for the model.\n",
    "    It first uses [`LayoutLMREFeatureExtractor`] to resize and normalize document images, and optionally applies OCR to\n",
    "    get words and normalized bounding boxes. These are then provided to [`LayoutLMRETokenizer`] or\n",
    "    [`LayoutLMRETokenizerFast`], which turns the words and bounding boxes into token-level `input_ids`,\n",
    "    `attention_mask`, `token_type_ids`, `bbox`. Optionally, one can provide integer `word_labels`, which are turned\n",
    "    into token-level `labels` for token classification tasks (such as FUNSD, CORD).\n",
    "    Args:\n",
    "        feature_extractor (`LayoutLMREFeatureExtractor`):\n",
    "            An instance of [`LayoutLMREFeatureExtractor`]. The feature extractor is a required input.\n",
    "        tokenizer (`LayoutLMRETokenizer` or `LayoutLMRETokenizerFast`):\n",
    "            An instance of [`LayoutLMRETokenizer`] or [`LayoutLMRETokenizerFast`]. The tokenizer is a required input.\n",
    "    \"\"\"\n",
    "    feature_extractor_class = (\"LayoutLMREFeatureExtractor\", 'LayoutLMREImageProcessor')\n",
    "    tokenizer_class = (\"LayoutLMRETokenizer\", \"LayoutLMRETokenizerFast\")\n",
    "    def __init__(\n",
    "        self,\n",
    "        feature_extractor,\n",
    "        tokenizer,\n",
    "    ):\n",
    "        self.feature_extractor = feature_extractor\n",
    "        self.tokenizer = tokenizer\n",
    "        \n",
    "    def __call__(\n",
    "        self,\n",
    "        images,\n",
    "        text: Union[TextInput, PreTokenizedInput, List[TextInput], List[PreTokenizedInput]] = None,\n",
    "        pair: Optional[str] = None,\n",
    "        text_pair: Optional[Union[PreTokenizedInput, List[PreTokenizedInput]]] = None,\n",
    "        boxes: Union[List[List[int]], List[List[List[int]]]] = None,\n",
    "        word_labels: Optional[Union[List[int], List[List[int]]]] = None,\n",
    "        add_special_tokens: bool = True,\n",
    "        padding: Union[bool, str, PaddingStrategy] = False,\n",
    "        truncation: Union[bool, str, TruncationStrategy] = None,\n",
    "        max_length: Optional[int] = None,\n",
    "        stride: int = 0,\n",
    "        pad_to_multiple_of: Optional[int] = None,\n",
    "        return_token_type_ids: Optional[bool] = None,\n",
    "        return_attention_mask: Optional[bool] = None,\n",
    "        return_overflowing_tokens: bool = False,\n",
    "        return_special_tokens_mask: bool = False,\n",
    "        return_offsets_mapping: bool = False,\n",
    "        return_length: bool = False,\n",
    "        verbose: bool = True,\n",
    "        return_tensors: Optional[Union[str, TensorType]] = None,\n",
    "        **kwargs\n",
    "    ) -> BatchEncoding:\n",
    "        \"\"\"\n",
    "        This method first forwards the `images` argument to [`~LayoutLMREFeatureExtractor.__call__`]. In case\n",
    "        [`LayoutLMREFeatureExtractor`] was initialized with `apply_ocr` set to `True`, it passes the obtained words and\n",
    "        bounding boxes along with the additional arguments to [`~LayoutLMRETokenizer.__call__`] and returns the output,\n",
    "        together with resized and normalized `pixel_values`. In case [`LayoutLMREFeatureExtractor`] was initialized\n",
    "        with `apply_ocr` set to `False`, it passes the words (`text`/``text_pair`) and `boxes` specified by the user\n",
    "        along with the additional arguments to [`~LayoutLMRETokenizer.__call__`] and returns the output, together with\n",
    "        resized and normalized `pixel_values`.\n",
    "        Please refer to the docstring of the above two methods for more information.\n",
    "        \"\"\"\n",
    "        # verify input\n",
    "        if self.feature_extractor.apply_ocr and (boxes is not None):\n",
    "            raise ValueError(\n",
    "                \"You cannot provide bounding boxes \"\n",
    "                \"if you initialized the feature extractor with apply_ocr set to True.\"\n",
    "            )\n",
    "\n",
    "        if self.feature_extractor.apply_ocr and (word_labels is not None):\n",
    "            raise ValueError(\n",
    "                \"You cannot provide word labels if you initialized the feature extractor with apply_ocr set to True.\"\n",
    "            )\n",
    "\n",
    "        # first, apply the feature extractor\n",
    "        features = self.feature_extractor(images=images, return_tensors=return_tensors)\n",
    "        boxes = [[a[0], b[1], c[0], d[1]] for [a,b,c,d] in boxes]\n",
    "        # second, apply the tokenizer\n",
    "        if text is not None and self.feature_extractor.apply_ocr and text_pair is None:\n",
    "            if isinstance(text, str):\n",
    "                text = [text]  # add batch dimension (as the feature extractor always adds a batch dimension)\n",
    "            text_pair = features[\"words\"]\n",
    "        if text and pair:\n",
    "            text,boxes, e1_e2 = self.process_text(text, boxes, pair)\n",
    "        encoded_inputs = self.tokenizer(\n",
    "            text=text if text is not None else features[\"words\"],\n",
    "            text_pair=text_pair if text_pair is not None else None,\n",
    "            boxes=boxes if boxes is not None else features[\"boxes\"],\n",
    "            word_labels=word_labels,\n",
    "            add_special_tokens=add_special_tokens,\n",
    "            padding=padding,\n",
    "            truncation=truncation,\n",
    "            max_length=max_length,\n",
    "            stride=stride,\n",
    "            pad_to_multiple_of=pad_to_multiple_of,\n",
    "            return_token_type_ids=return_token_type_ids,\n",
    "            return_attention_mask=return_attention_mask,\n",
    "            return_overflowing_tokens=return_overflowing_tokens,\n",
    "            return_special_tokens_mask=return_special_tokens_mask,\n",
    "            return_offsets_mapping=return_offsets_mapping,\n",
    "            return_length=return_length,\n",
    "            verbose=verbose,\n",
    "            return_tensors=return_tensors,\n",
    "            **kwargs,\n",
    "        )\n",
    "\n",
    "        # add pixel values\n",
    "        images = features.pop(\"pixel_values\")\n",
    "        if return_overflowing_tokens is True:\n",
    "            images = self.get_overflowing_images(images, encoded_inputs[\"overflow_to_sample_mapping\"])\n",
    "        encoded_inputs[\"pixel_values\"] = images\n",
    "\n",
    "        return encoded_inputs\n",
    "    def process_text(self, text, boxes, pair):\n",
    "        e1_idx = text.index(pair[0])\n",
    "        text.insert(e1_idx, '<E1>')\n",
    "        boxes.insert(e1_idx, boxes[e1_idx])\n",
    "        text.insert(e1_idx+2, '</E1>')\n",
    "        boxes.insert(e1_idx+2, boxes[e1_idx+1])\n",
    "        e2_idx = text.index(pair[1])\n",
    "        text.insert(e2_idx, '<E2>')\n",
    "        boxes.insert(e2_idx, boxes[e2_idx])\n",
    "        text.insert(e2_idx+2, '</E2>')\n",
    "        boxes.insert(e2_idx+2, boxes[e2_idx+1])\n",
    "        pair = (text.index('<E1>'), text.index('<E2>'))\n",
    "        return text, boxes, pair\n",
    "        \n",
    "    def get_overflowing_images(self, images, overflow_to_sample_mapping):\n",
    "        # in case there's an overflow, ensure each `input_ids` sample is mapped to its corresponding image\n",
    "        images_with_overflow = []\n",
    "        for sample_idx in overflow_to_sample_mapping:\n",
    "            images_with_overflow.append(images[sample_idx])\n",
    "\n",
    "        if len(images_with_overflow) != len(overflow_to_sample_mapping):\n",
    "            raise ValueError(\n",
    "                \"Expected length of images to be the same as the length of `overflow_to_sample_mapping`, but got\"\n",
    "                f\" {len(images_with_overflow)} and {len(overflow_to_sample_mapping)}\"\n",
    "            )\n",
    "\n",
    "        return images_with_overflow\n",
    "\n",
    "    def batch_decode(self, *args, **kwargs):\n",
    "        \"\"\"\n",
    "        This method forwards all its arguments to PreTrainedTokenizer's [`~PreTrainedTokenizer.batch_decode`]. Please\n",
    "        refer to the docstring of this method for more information.\n",
    "        \"\"\"\n",
    "        return self.tokenizer.batch_decode(*args, **kwargs)\n",
    "\n",
    "    def decode(self, *args, **kwargs):\n",
    "        \"\"\"\n",
    "        This method forwards all its arguments to PreTrainedTokenizer's [`~PreTrainedTokenizer.decode`]. Please refer\n",
    "        to the docstring of this method for more information.\n",
    "        \"\"\"\n",
    "        return self.tokenizer.decode(*args, **kwargs)\n",
    "\n",
    "    @property\n",
    "    def model_input_names(self):\n",
    "        return [\"input_ids\", \"bbox\", \"attention_mask\", \"pixel_values\"]"
   ]
  },
  {
   "attachments": {},
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### test process"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 48,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "dict_keys(['ocr-path', 'relation-pair', 'ocr-token', 'ocr-bbox', 'image_path', 'image', 'pair', 'class-label', 'e1_e2'])\n"
     ]
    }
   ],
   "source": [
    "from transformers import AutoTokenizer\n",
    "tokenizer = AutoTokenizer.from_pretrained(\"microsoft/layoutlmv3-base\", cache_dir = cachedir)\n",
    "# data.push_to_hub('Hantao/ChemProcessed')\n",
    "\n",
    "print(example.keys())"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 34,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "[[[299.0, 41.0], [317.0, 41.0], [317.0, 53.0], [299.0, 53.0]],\n",
       " [[299.0, 83.0], [346.0, 85.0], [346.0, 101.0], [299.0, 98.0]],\n",
       " [[231.0, 171.0], [295.0, 171.0], [295.0, 188.0], [231.0, 188.0]],\n",
       " [[351.0, 170.0], [411.0, 167.0], [412.0, 185.0], [352.0, 189.0]],\n",
       " [[3.0, 216.0], [26.0, 216.0], [26.0, 227.0], [3.0, 227.0]],\n",
       " [[429.0, 215.0], [453.0, 215.0], [453.0, 227.0], [429.0, 227.0]],\n",
       " [[1.0, 244.0], [208.0, 244.0], [208.0, 258.0], [1.0, 258.0]],\n",
       " [[453.0, 244.0], [614.0, 244.0], [614.0, 258.0], [453.0, 258.0]]]"
      ]
     },
     "execution_count": 34,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "token_list = [\n",
    "    '<E1>',\n",
    "    '<E2>',\n",
    "    '</E1>',\n",
    "    '</E2>',\n",
    "]\n",
    "tokenizer.add_tokens(token_list)\n",
    "image = data[0]['image']\n",
    "data[0]['ocr-bbox']"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 35,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "<ChannelDimension.LAST: 'channels_last'>"
      ]
     },
     "execution_count": 35,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "from detectron2.data.detection_utils import read_image, convert_PIL_to_numpy\n",
    "from transformers.image_utils import infer_channel_dimension_format\n",
    "vec = convert_PIL_to_numpy(image, None)\n",
    "# infer_channel_dimension_format(vec) # cause some bug here\n",
    "vec.ndim\n",
    "vec.shape[2]\n",
    "new_image = image.convert('RGB')\n",
    "vec = convert_PIL_to_numpy(new_image, None)\n",
    "infer_channel_dimension_format(vec)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 46,
   "metadata": {},
   "outputs": [],
   "source": [
    "\n",
    "LayoutLMREFeatureExtractor = LayoutLMREImageProcessor\n",
    "extractor = LayoutLMREFeatureExtractor()\n",
    "example = data[1]\n",
    "extractor.apply_ocr = False\n",
    "processor = LayoutLMREProcessor(feature_extractor = extractor, tokenizer = tokenizer)\n",
    "coded = processor(\n",
    "    images = example['image'].convert('RGB'),\n",
    "    text = example['ocr-token'],\n",
    "    boxes = example['ocr-bbox'],\n",
    "    pair = example['pair'],\n",
    "    return_tensors='pt',\n",
    ")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 47,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "{'input_ids': tensor([[    0,  1368,   705,  8765,  3411, 44174,  7561, 44174,   510,  2744,\n",
       "         31528, 31528, 50266, 24892,  4306, 10159, 39305,   890,  1949,  1640,\n",
       "           642,   611,   462,  1949, 50268, 50265, 36846, 39305,   890,  1949,\n",
       "          1640,   611,   462,  1949,    43, 50267,     2]]), 'attention_mask': tensor([[1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1,\n",
       "         1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1]]), 'bbox': tensor([[[  0.,   0.,   0.,   0.],\n",
       "         [299.,  41., 317.,  53.],\n",
       "         [299.,  41., 317.,  53.],\n",
       "         [299.,  85., 346.,  98.],\n",
       "         [299.,  85., 346.,  98.],\n",
       "         [231., 171., 295., 188.],\n",
       "         [231., 171., 295., 188.],\n",
       "         [351., 167., 412., 189.],\n",
       "         [351., 167., 412., 189.],\n",
       "         [351., 167., 412., 189.],\n",
       "         [  3., 216.,  26., 227.],\n",
       "         [429., 215., 453., 227.],\n",
       "         [  1., 244., 208., 258.],\n",
       "         [  1., 244., 208., 258.],\n",
       "         [  1., 244., 208., 258.],\n",
       "         [  1., 244., 208., 258.],\n",
       "         [  1., 244., 208., 258.],\n",
       "         [  1., 244., 208., 258.],\n",
       "         [  1., 244., 208., 258.],\n",
       "         [  1., 244., 208., 258.],\n",
       "         [  1., 244., 208., 258.],\n",
       "         [  1., 244., 208., 258.],\n",
       "         [  1., 244., 208., 258.],\n",
       "         [  1., 244., 208., 258.],\n",
       "         [  1., 244., 208., 258.],\n",
       "         [453., 244., 614., 258.],\n",
       "         [453., 244., 614., 258.],\n",
       "         [453., 244., 614., 258.],\n",
       "         [453., 244., 614., 258.],\n",
       "         [453., 244., 614., 258.],\n",
       "         [453., 244., 614., 258.],\n",
       "         [453., 244., 614., 258.],\n",
       "         [453., 244., 614., 258.],\n",
       "         [453., 244., 614., 258.],\n",
       "         [453., 244., 614., 258.],\n",
       "         [453., 244., 614., 258.],\n",
       "         [  0.,   0.,   0.,   0.]]]), 'pixel_values': tensor([[[[ 1.0000,  1.0000,  1.0000,  ...,  1.0000,  1.0000,  1.0000],\n",
       "          [ 1.0000,  1.0000,  1.0000,  ...,  1.0000,  1.0000,  1.0000],\n",
       "          [ 1.0000,  1.0000,  1.0000,  ...,  1.0000,  1.0000,  1.0000],\n",
       "          ...,\n",
       "          [-0.0588,  0.9451,  0.9843,  ...,  1.0000,  1.0000,  1.0000],\n",
       "          [ 0.2471,  0.9608,  0.9922,  ...,  1.0000,  1.0000,  1.0000],\n",
       "          [ 0.8824,  0.9765,  0.9765,  ...,  1.0000,  1.0000,  1.0000]],\n",
       "\n",
       "         [[ 1.0000,  1.0000,  1.0000,  ...,  1.0000,  1.0000,  1.0000],\n",
       "          [ 1.0000,  1.0000,  1.0000,  ...,  1.0000,  1.0000,  1.0000],\n",
       "          [ 1.0000,  1.0000,  1.0000,  ...,  1.0000,  1.0000,  1.0000],\n",
       "          ...,\n",
       "          [-0.0588,  0.9451,  0.9843,  ...,  1.0000,  1.0000,  1.0000],\n",
       "          [ 0.2471,  0.9608,  0.9922,  ...,  1.0000,  1.0000,  1.0000],\n",
       "          [ 0.8824,  0.9765,  0.9765,  ...,  1.0000,  1.0000,  1.0000]],\n",
       "\n",
       "         [[ 1.0000,  1.0000,  1.0000,  ...,  1.0000,  1.0000,  1.0000],\n",
       "          [ 1.0000,  1.0000,  1.0000,  ...,  1.0000,  1.0000,  1.0000],\n",
       "          [ 1.0000,  1.0000,  1.0000,  ...,  1.0000,  1.0000,  1.0000],\n",
       "          ...,\n",
       "          [-0.0588,  0.9451,  0.9843,  ...,  1.0000,  1.0000,  1.0000],\n",
       "          [ 0.2471,  0.9608,  0.9922,  ...,  1.0000,  1.0000,  1.0000],\n",
       "          [ 0.8824,  0.9765,  0.9765,  ...,  1.0000,  1.0000,  1.0000]]]])}"
      ]
     },
     "execution_count": 47,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "coded"
   ]
  },
  {
   "attachments": {},
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## datamatching learned from plkmo/BRE\n",
    "\n",
    "So after a long time search and using `debugpy` to step by step learn the proram, the first complicated one was just a wrong guessing lol\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 38,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "dict_keys(['ocr-path', 'relation-pair', 'ocr-token', 'ocr-bbox', 'image_path', 'image', 'pair', 'class-label', 'e1_e2'])\n",
      "['chlorophyllide(chlide)', 'protochlorophyllide(pchlide']\n",
      "7\n",
      "['hv', 'LPOR', 'NADPH', 'NADP+', 'HO', 'HO', 'protochlorophyllide(pchlide', 'chlorophyllide(chlide)']\n"
     ]
    }
   ],
   "source": [
    "e1_start_id = tokenizer.convert_tokens_to_ids('<E1>')\n",
    "e2_start_id = tokenizer.convert_tokens_to_ids('<E2>')\n",
    "text = example['ocr-token']\n",
    "print(example.keys())\n",
    "print(example['pair'])\n",
    "print(text.index(example['pair'][0]))\n",
    "# text.insert(text.index(example['pair'][0]), '<E1>' )\n",
    "print(example['ocr-token'])"
   ]
  },
  {
   "attachments": {},
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# train and loss computations"
   ]
  },
  {
   "attachments": {},
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## return data"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "from dataclasses import dataclass\n",
    "from typing import Optional, Tuple\n",
    "\n",
    "import torch\n",
    "\n",
    "from transformers.file_utils import ModelOutput\n",
    "\n",
    "\n",
    "\n",
    "class BaseModelOutput(ModelOutput):\n",
    "    \"\"\"\n",
    "    Base class for model's outputs, with potential hidden states and attentions.\n",
    "\n",
    "    Args:\n",
    "        last_hidden_state (:obj:`torch.FloatTensor` of shape :obj:`(batch_size, sequence_length, hidden_size)`):\n",
    "            Sequence of hidden-states at the output of the last layer of the model.\n",
    "        hidden_states (:obj:`tuple(torch.FloatTensor)`, `optional`, returned when ``output_hidden_states=True`` is passed or when ``config.output_hidden_states=True``):\n",
    "            Tuple of :obj:`torch.FloatTensor` (one for the output of the embeddings + one for the output of each layer)\n",
    "            of shape :obj:`(batch_size, sequence_length, hidden_size)`.\n",
    "\n",
    "            Hidden-states of the model at the output of each layer plus the initial embedding outputs.\n",
    "        attentions (:obj:`tuple(torch.FloatTensor)`, `optional`, returned when ``output_attentions=True`` is passed or when ``config.output_attentions=True``):\n",
    "            Tuple of :obj:`torch.FloatTensor` (one for each layer) of shape :obj:`(batch_size, num_heads,\n",
    "            sequence_length, sequence_length)`.\n",
    "\n",
    "            Attentions weights after the attention softmax, used to compute the weighted average in the self-attention\n",
    "            heads.\n",
    "    \"\"\"\n",
    "\n",
    "    last_hidden_state: torch.FloatTensor = None\n",
    "    hidden_states: Optional[Tuple[torch.FloatTensor]] = None\n",
    "    attentions: Optional[Tuple[torch.FloatTensor]] = None\n",
    "    loss: Optional[torch.FloatTensor] = None,\n",
    "    "
   ]
  },
  {
   "attachments": {},
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## model writings"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "import math\n",
    "from typing import Optional, Tuple, Union\n",
    "\n",
    "\n",
    "import math\n",
    "from typing import Optional, Tuple, Union\n",
    "\n",
    "import torch\n",
    "import torch.utils.checkpoint\n",
    "from torch import nn\n",
    "from torch.nn import BCEWithLogitsLoss, CrossEntropyLoss, MSELoss\n",
    "\n",
    "from transformers.activations import ACT2FN\n",
    "from transformers.modeling_outputs import (\n",
    "    BaseModelOutputWithPastAndCrossAttentions,\n",
    "    BaseModelOutputWithPoolingAndCrossAttentions,\n",
    "    MaskedLMOutput,\n",
    "    QuestionAnsweringModelOutput,\n",
    "    SequenceClassifierOutput,\n",
    "    TokenClassifierOutput,\n",
    ")\n",
    "from transformers.modeling_utils import PreTrainedModel\n",
    "from transformers.pytorch_utils import apply_chunking_to_forward, find_pruneable_heads_and_indices, prune_linear_layer\n",
    "from transformers.utils import add_start_docstrings, add_start_docstrings_to_model_forward, logging, replace_return_docstrings\n",
    "from transformers import LayoutLMConfig\n",
    "\n",
    "\n",
    "logger = logging.get_logger(__name__)\n",
    "\n",
    "_CONFIG_FOR_DOC = \"LayoutLMConfig\"\n",
    "_CHECKPOINT_FOR_DOC = \"microsoft/layoutlm-base-uncased\"\n",
    "\n",
    "LAYOUTLM_PRETRAINED_MODEL_ARCHIVE_LIST = [\n",
    "    \"layoutlm-base-uncased\",\n",
    "    \"layoutlm-large-uncased\",\n",
    "]\n",
    "\n",
    "\n",
    "LayoutLMLayerNorm = nn.LayerNorm\n",
    "\n",
    "from transformers import LayoutLMPreTrainedModel, LayoutLMModel\n",
    "from transformers.utils import (\n",
    "    add_start_docstrings, add_start_docstrings_to_model_forward, logging, replace_return_docstrings\n",
    ")\n",
    "\n",
    "class LayoutLMRE(LayoutLMPreTrainedModel):\n",
    "    def __init__(self, config):\n",
    "        super().__init__(config)\n",
    "        self.num_labels = config.num_labels\n",
    "        self.layoutlm = LayoutLMModel(config)\n",
    "        self.dropout = nn.Dropout(config.hidden_dropout_prob)\n",
    "        self.classifier = nn.Linear(config.hidden_size, config.num_labels)\n",
    "        self.task = 'classification'\n",
    "\n",
    "        # Initialize weights and apply final processing\n",
    "        self.post_init()\n",
    "        self.criterion = Two_Headed_Loss(lm_ignore_idx=config.pad_token_id, use_logits=True, normalize=False)\n",
    "    def get_input_embeddings(self):\n",
    "        return self.layoutlm.embeddings.word_embeddings\n",
    "\n",
    "    # @add_start_docstrings_to_model_forward(LAYOUTLM_INPUTS_DOCSTRING.format(\"batch_size, sequence_length\"))\n",
    "    @replace_return_docstrings(output_type=TokenClassifierOutput, config_class=_CONFIG_FOR_DOC)\n",
    "    def forward(\n",
    "        self,\n",
    "        input_ids: Optional[torch.LongTensor] = None,\n",
    "        bbox: Optional[torch.LongTensor] = None,\n",
    "        attention_mask: Optional[torch.FloatTensor] = None,\n",
    "        token_type_ids: Optional[torch.LongTensor] = None,\n",
    "        position_ids: Optional[torch.LongTensor] = None,\n",
    "        head_mask: Optional[torch.FloatTensor] = None,\n",
    "        inputs_embeds: Optional[torch.FloatTensor] = None,\n",
    "        labels: Optional[torch.LongTensor] = None,\n",
    "        output_attentions: Optional[bool] = None,\n",
    "        output_hidden_states: Optional[bool] = None,\n",
    "        return_dict: Optional[bool] = None,\n",
    "        e1_e2_start: Optional[int] = None,\n",
    "        mask_id: int = 0,\n",
    "    ) -> Union[Tuple, TokenClassifierOutput]:\n",
    "        r\"\"\"\n",
    "        labels (`torch.LongTensor` of shape `(batch_size, sequence_length)`, *optional*):\n",
    "            Labels for computing the token classification loss. Indices should be in `[0, transformers., config.num_labels - 1]`.\n",
    "        Returns:\n",
    "        Examples:\n",
    "        ```python\n",
    "        >>> from transformers import AutoTokenizer, LayoutLMForTokenClassification\n",
    "        >>> import torch\n",
    "        >>> tokenizer = AutoTokenizer.from_pretrained(\"microsoft/layoutlm-base-uncased\")\n",
    "        >>> model = LayoutLMForTokenClassification.from_pretrained(\"microsoft/layoutlm-base-uncased\")\n",
    "        >>> words = [\"Hello\", \"world\"]\n",
    "        >>> normalized_word_boxes = [637, 773, 693, 782], [698, 773, 733, 782]\n",
    "        >>> token_boxes = []\n",
    "        >>> for word, box in zip(words, normalized_word_boxes):\n",
    "        transformers.     word_tokens = tokenizer.tokenize(word)\n",
    "        transformers.     token_boxes.extend([box] * len(word_tokens))\n",
    "        >>> # add bounding boxes of cls + sep tokens\n",
    "        >>> token_boxes = [[0, 0, 0, 0]] + token_boxes + [[1000, 1000, 1000, 1000]]\n",
    "        >>> encoding = tokenizer(\" \".join(words), return_tensors=\"pt\")\n",
    "        >>> input_ids = encoding[\"input_ids\"]\n",
    "        >>> attention_mask = encoding[\"attention_mask\"]\n",
    "        >>> token_type_ids = encoding[\"token_type_ids\"]\n",
    "        >>> bbox = torch.tensor([token_boxes])\n",
    "        >>> token_labels = torch.tensor([1, 1, 0, 0]).unsqueeze(0)  # batch size of 1\n",
    "        >>> outputs = model(\n",
    "        transformers.     input_ids=input_ids,\n",
    "        transformers.     bbox=bbox,\n",
    "        transformers.     attention_mask=attention_mask,\n",
    "        transformers.     token_type_ids=token_type_ids,\n",
    "        transformers.     labels=token_labels,\n",
    "        transformers. )\n",
    "        >>> loss = outputs.loss\n",
    "        >>> logits = outputs.logits\n",
    "        ```\"\"\"\n",
    "        return_dict = return_dict if return_dict is not None else self.config.use_return_dict\n",
    "\n",
    "        outputs = self.layoutlm(\n",
    "            input_ids=input_ids,\n",
    "            bbox=bbox,\n",
    "            attention_mask=attention_mask,\n",
    "            token_type_ids=token_type_ids,\n",
    "            position_ids=position_ids,\n",
    "            head_mask=head_mask,\n",
    "            inputs_embeds=inputs_embeds,\n",
    "            output_attentions=output_attentions,\n",
    "            output_hidden_states=output_hidden_states,\n",
    "            return_dict=return_dict,\n",
    "        )\n",
    "\n",
    "        sequence_output = outputs[0]\n",
    "        blankv1v2 = sequence_output[:, e1_e2_start, :]\n",
    "        buffer = []\n",
    "        for i in range(blankv1v2.shape[0]): # iterate batch & collect\n",
    "            v1v2 = blankv1v2[i, i, :, :]\n",
    "            v1v2 = torch.cat((v1v2[0], v1v2[1]))\n",
    "            buffer.append(v1v2)\n",
    "        del blankv1v2\n",
    "        v1v2 = torch.stack([a for a in buffer], dim=0)\n",
    "        del buffer\n",
    "        \n",
    "        loss = None\n",
    "        if self.task is None:\n",
    "            blanks_logits = self.activation(v1v2) # self.blanks_linear(- torch.log(Q)\n",
    "            lm_logits = self.cls(sequence_output)\n",
    "            lm_logits = lm_logits[(input_ids == mask_id)]\n",
    "            x, masked_for_pred, e1_e2_start, _, blank_labels, _,_,_,_,_ = data\n",
    "            \n",
    "            \n",
    "            loss = self.criterion(lm_logits, blanks_logits, masked_for_pred, blank_labels, verbose=verbose)\n",
    "            # return blanks_logits, lm_logits\n",
    "\n",
    "        classification_logits = self.classifier(v1v2)\n",
    "\n",
    "\n",
    "\n",
    "        loss = None\n",
    "        if labels is not None:\n",
    "            loss_fct = CrossEntropyLoss()\n",
    "            loss = loss_fct(logits.view(-1, self.num_labels), labels.view(-1))\n",
    "\n",
    "        if not return_dict:\n",
    "            output = (logits,) + outputs[2:]\n",
    "            return ((loss,) + output) if loss is not None else output\n",
    "\n",
    "        "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "env",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.10.8"
  },
  "orig_nbformat": 4,
  "vscode": {
   "interpreter": {
    "hash": "8478f3a2d2984c069ac3bad78fb220d35bf3c0da0900f2e0dedcf3b0d1a57c6d"
   }
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
